from transformers import AutoTokenizer, AutoModelForCausalLM
import sys
import json

# Load the tokenizer and model from Hugging Face
model_name = "microsoft/Phi-3.5-mini-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

def generate_response(prompt):
    try:
        # Prepend a reset command or instruction to clarify context for the model
        # prompt = f"Please respond to the following question independently:\n{input_text}"

        # Encode the input text
        inputs = tokenizer(prompt, return_tensors="pt")
        
        # Generate a response with adjusted parameters
        outputs = model.generate(
            **inputs,
            max_length=100,
            do_sample=False,
            num_return_sequences=1,
            pad_token_id=tokenizer.eos_token_id
        )

        # Check if outputs tensor has at least one token
        if outputs.size(0) > 0:
            # Decode and return the response
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            return response
        else:
            return "No response generated by the model."

    except Exception as e:
        return f"An error occurred: {str(e)}"


def main():
    print("Python model ready2")  # Signal to C# that Python is ready
    sys.stdout.flush()

    while True:
        print("waiting...")  # Signal to C# that Python is ready
        # Read input from standard input
        input_line = sys.stdin.readline().strip()
        
        print(f"input_line - {input_line}")
        
        # Exit condition
        if input_line == "exit":
            print("Python exiting")
            sys.stdout.flush()
            break
        
        # Generate response
        prompt = f"Please respond to the following question independently:\n{input_line}"
        response = generate_response(prompt)
        # response = generate_response(input_line)
        
        # Ensure both `prompt` and `response` are included in the output
        output = {
            "prompt": prompt,
            "response": response
        }

        # Debugging information (optional)
        sys.stderr.write(f"DEBUG: Output generated: {output}\n")
        sys.stderr.flush()

        # Output the response as JSON, including the prompt
        print(json.dumps(output))
        sys.stdout.flush()

if __name__ == "__main__":
    # Check if running in an interactive console
    print("Starting python script...")

    if sys.stdin.isatty():
        # Interactive mode
        print("Running in interactive mode. Type 'exit' to quit2.")
        while True:
            input_text = input("You: ")
            if input_text.lower() == "exit":
                print("Exiting interactive mode.")
                break
            
            prompt = f"Please respond to the following question independently:\n{input_text}"
            response = generate_response(prompt)
            output = {
                "prompt": prompt,
                "response": response
            }
            #print("Model:", json.dumps(output))
            print("Model:", "blow me")
    else:
        # Running from the C# command-line app
        print("Running in main mode2.")
        main()
